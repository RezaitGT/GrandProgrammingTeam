from flask import Flask, render_template, request, jsonify
import os
import uuid
from werkzeug.utils import secure_filename
import re
from datetime import datetime
import fitz  # PyMuPDF
import math

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['SECRET_KEY'] = 'normcontrol-secret-key-2024'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# =============================================================================
# CONFIGURATION
# =============================================================================
class Config:
    DOCUMENT_CODES = {
        '–°–ë': '–°–±–æ—Ä–æ—á–Ω—ã–π —á–µ—Ä—Ç–µ–∂',
        '–í–û': '–ß–µ—Ä—Ç–µ–∂ –æ–±—â–µ–≥–æ –≤–∏–¥–∞', 
        '–¢–ß': '–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π —á–µ—Ä—Ç–µ–∂',
        '–ì–ß': '–ì–∞–±–∞—Ä–∏—Ç–Ω—ã–π —á–µ—Ä—Ç–µ–∂',
        '–ú–≠': '–≠–ª–µ–∫—Ç—Ä–æ–º–æ–Ω—Ç–∞–∂–Ω—ã–π —á–µ—Ä—Ç–µ–∂',
        '–ú–ß': '–ú–æ–Ω—Ç–∞–∂–Ω—ã–π —á–µ—Ä—Ç–µ–∂',
        '–£–ß': '–£–ø–∞–∫–æ–≤–æ—á–Ω—ã–π —á–µ—Ä—Ç–µ–∂',
        '–í–°': '–í–µ–¥–æ–º–æ—Å—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π',
        '–≠3': '–°—Ö–µ–º–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–∞—è',
        '–≠4': '–°—Ö–µ–º–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π', 
        '–≠5': '–°—Ö–µ–º–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è'
    }

    TOLERANCE_SYMBOLS = ['‚èä', '‚ä•', '‚à•', '‚à†', '‚óã', '‚åí', '‚èã']
    BASE_SEPARATOR = '‚Äî'

# =============================================================================
# PRECISE DOCUMENT ANALYZER
# =============================================================================
class DocumentAnalyzer:
    def extract_text_from_pdf(self, pdf_path: str) -> dict:
        """–¢–æ—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        try:
            doc = fitz.open(pdf_path)
            text_data = {'pages': [], 'total_pages': doc.page_count}
            
            # –°–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            first_page_tech_requirements = ""
            if doc.page_count > 0:
                first_page = doc[0]
                first_page_text = first_page.get_text("text", sort=True)
                first_page_dict = first_page.get_text("dict", sort=True)
                first_page_width = first_page.rect.width
                first_page_height = first_page.rect.height
                
                first_page_tech_requirements = self._extract_tech_requirements_improved(
                    first_page_dict, first_page_text, first_page_width, first_page_height
                )['text']
                
                print(f"\nüìã –¢–ï–•–¢–†–ï–ë–û–í–ê–ù–ò–Ø –° –ü–ï–†–í–û–ô –°–¢–†–ê–ù–ò–¶–´:")
                print(f"'{first_page_tech_requirements[:200]}...'")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            for page_num in range(doc.page_count):
                page = doc[page_num]
                
                raw_text = page.get_text("text", sort=True)
                text_dict = page.get_text("dict", sort=True)
                width = page.rect.width
                height = page.rect.height
                drawings = page.get_drawings()
                
                print(f"\nüìÑ –°–¢–†–ê–ù–ò–¶–ê {page_num + 1} ({width}x{height})")
                print("=" * 50)
                
                # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                try:
                    analysis = self._analyze_page_details(
                        text_dict, raw_text, width, height, drawings, page, 
                        first_page_tech_requirements if page_num == 0 else "",
                        page_num + 1
                    )
                except Exception as e:
                    print(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    return {'pages': [], 'total_pages': 0, 'error': str(e)}
                
                text_data['pages'].append({
                    'page_number': page_num + 1,
                    'width': width,
                    'height': height,
                    'raw_text': raw_text,
                    'text_dict': text_dict,
                    'drawings': drawings,
                    'analysis': analysis
                })
            
            doc.close()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            text_data['first_page_tech_requirements'] = first_page_tech_requirements
            return text_data
        except Exception as e:
            return {'pages': [], 'total_pages': 0, 'error': str(e)}

    def _analyze_page_details(self, text_dict: dict, raw_text: str, width: float, height: float, 
                            drawings: list, page, first_page_tech_requirements: str = "", page_num: int = 1) -> dict:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        analysis = {
            'title_block': self._extract_title_block_improved(text_dict, width, height),
            'drawing_area': self._extract_drawing_area_improved(text_dict, width, height),
            'tech_requirements': {'text': '', 'lines': [], 'spans': []},  # –ü—É—Å—Ç—ã–µ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–π
            'found_elements': {
                'codes': [],
                'letters': [],
                'asterisks': {'single': [], 'double': [], 'triple': []},
                'dimensions': [],
                'tolerances': [],
                'bases': [],
                'arrows': [],
                'lines': []
            }
        }
        
        # –î–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        if page_num == 1:
            analysis['tech_requirements'] = self._extract_tech_requirements_improved(text_dict, raw_text, width, height)
        else:
            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            analysis['tech_requirements'] = {
                'text': first_page_tech_requirements,
                'lines': first_page_tech_requirements.split('\n') if first_page_tech_requirements else [],
                'spans': []
            }
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        all_text = analysis['title_block']['text'] + " " + analysis['drawing_area']['text']
        analysis['found_elements'] = self._analyze_elements(
            all_text, 
            first_page_tech_requirements,  # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_num
        )
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        analysis['graphic_analysis'] = self._analyze_graphic_elements(drawings, text_dict, page)
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        self._print_detailed_diagnostics(analysis, width, height, page_num)
        
        return analysis

    def _extract_title_block_improved(self, text_dict: dict, width: float, height: float) -> dict:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏"""
        title_spans = []
        title_text = ""
        
        # –û—Å–Ω–æ–≤–Ω–∞—è –Ω–∞–¥–ø–∏—Å—å –æ–±—ã—á–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø—Ä–∞–≤–æ–º –Ω–∏–∂–Ω–µ–º —É–≥–ª—É (–ì–û–°–¢ 2.104-2006)
        title_block_area = {
            'x_min': width * 0.6,   # –ü—Ä–∞–≤–∞—è —á–∞—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            'x_max': width,
            'y_min': height * 0.7,  # –ù–∏–∂–Ω—è—è —á–∞—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã  
            'y_max': height
        }
        
        print(f"üìç –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏ –≤ –æ–±–ª–∞—Å—Ç–∏: {title_block_area}")
        
        for block in text_dict.get('blocks', []):
            if block['type'] == 0:  # –¢–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫
                for line in block['lines']:
                    for span in line['spans']:
                        bbox = span['bbox']
                        span_center_x = (bbox[0] + bbox[2]) / 2
                        span_center_y = (bbox[1] + bbox[3]) / 2
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ span –≤ –æ–±–ª–∞—Å—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏
                        if (title_block_area['x_min'] <= span_center_x <= title_block_area['x_max'] and
                            title_block_area['y_min'] <= span_center_y <= title_block_area['y_max']):
                            
                            text = span.get('text', '').strip()
                            if text:
                                title_spans.append({
                                    'text': text,
                                    'bbox': bbox,
                                    'position': (span_center_x, span_center_y)
                                })
                                title_text += text + " "
                                print(f"  üìç –ù–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏: '{text}' –≤ –ø–æ–∑–∏—Ü–∏–∏ ({span_center_x:.1f}, {span_center_y:.1f})")
        
        return {'text': title_text.strip(), 'spans': title_spans}

    def _extract_drawing_area_improved(self, text_dict: dict, width: float, height: float) -> dict:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª—è —á–µ—Ä—Ç–µ–∂–∞"""
        drawing_spans = []
        drawing_text = ""
        
        # –û–±–ª–∞—Å—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π (–ø—Ä–∞–≤–∞—è —á–∞—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã) - –£–í–ï–õ–ò–ß–ï–ù–ê
        tech_requirements_area = {
            'x_min': width * 0.55,   # –£–í–ï–õ–ò–ß–ï–ù–û: –±—ã–ª–æ 0.6
            'x_max': width,
            'y_min': 0,
            'y_max': height * 0.65   # –£–í–ï–õ–ò–ß–ï–ù–û: –±—ã–ª–æ 0.6
        }
        
        # –û–±–ª–∞—Å—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏ (–ø—Ä–∞–≤—ã–π –Ω–∏–∂–Ω–∏–π —É–≥–æ–ª)
        title_block_area = {
            'x_min': width * 0.6,
            'x_max': width, 
            'y_min': height * 0.7,
            'y_max': height
        }
        
        print(f"üìç –ü–æ–∏—Å–∫ –ø–æ–ª—è —á–µ—Ä—Ç–µ–∂–∞ (–∏—Å–∫–ª—é—á–∞—è —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –æ—Å–Ω–æ–≤–Ω—É—é –Ω–∞–¥–ø–∏—Å—å)")
        
        for block in text_dict.get('blocks', []):
            if block['type'] == 0:
                for line in block['lines']:
                    for span in line['spans']:
                        bbox = span['bbox']
                        span_center_x = (bbox[0] + bbox[2]) / 2
                        span_center_y = (bbox[1] + bbox[3]) / 2
                        
                        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –æ—Å–Ω–æ–≤–Ω—É—é –Ω–∞–¥–ø–∏—Å—å
                        in_tech_area = (tech_requirements_area['x_min'] <= span_center_x <= tech_requirements_area['x_max'] and
                                      tech_requirements_area['y_min'] <= span_center_y <= tech_requirements_area['y_max'])
                        
                        in_title_area = (title_block_area['x_min'] <= span_center_x <= title_block_area['x_max'] and
                                       title_block_area['y_min'] <= span_center_y <= title_block_area['y_max'])
                        
                        if not in_tech_area and not in_title_area:
                            text = span.get('text', '').strip()
                            if text:
                                drawing_spans.append({
                                    'text': text,
                                    'bbox': bbox,
                                    'position': (span_center_x, span_center_y)
                                })
                                drawing_text += text + " "
        
        return {'text': drawing_text.strip(), 'spans': drawing_spans}

    def _extract_tech_requirements_improved(self, text_dict: dict, raw_text: str, width: float, height: float) -> dict:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π - –ü–†–ê–í–ê–Ø –ß–ê–°–¢–¨ –°–¢–†–ê–ù–ò–¶–´"""
        tech_spans = []
        tech_text = ""
        tech_lines = []
        
        # –û–ë–õ–ê–°–¢–¨ –¢–ï–•–ù–ò–ß–ï–°–ö–ò–• –¢–†–ï–ë–û–í–ê–ù–ò–ô - –ü–†–ê–í–ê–Ø –ß–ê–°–¢–¨ –°–¢–†–ê–ù–ò–¶–´
        tech_requirements_area = {
            'x_min': width * 0.55,
            'x_max': width,
            'y_min': 0, 
            'y_max': height * 0.65
        }
        
        print(f"üìç –ü–æ–∏—Å–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏: {tech_requirements_area}")
        
        # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ –æ–±–ª–∞—Å—Ç–∏ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        all_tech_texts = []
        
        for block in text_dict.get('blocks', []):
            if block['type'] == 0:
                for line in block['lines']:
                    for span in line['spans']:
                        bbox = span['bbox']
                        span_center_x = (bbox[0] + bbox[2]) / 2
                        span_center_y = (bbox[1] + bbox[3]) / 2
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ span –≤ –æ–±–ª–∞—Å—Ç–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
                        if (tech_requirements_area['x_min'] <= span_center_x <= tech_requirements_area['x_max'] and
                            tech_requirements_area['y_min'] <= span_center_y <= tech_requirements_area['y_max']):
                            
                            text = span.get('text', '').strip()
                            if text:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï —Ç–µ–∫—Å—Ç—ã, –¥–∞–∂–µ –∫–æ—Ä–æ—Ç–∫–∏–µ
                                tech_spans.append({
                                    'text': text,
                                    'bbox': bbox,
                                    'position': (span_center_x, span_center_y)
                                })
                                all_tech_texts.append({
                                    'text': text,
                                    'y_position': span_center_y
                                })
                                print(f"  üìç –ù–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –≤ –æ–±–ª–∞—Å—Ç–∏ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: '{text}' –≤ –ø–æ–∑–∏—Ü–∏–∏ ({span_center_x:.1f}, {span_center_y:.1f})")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã –ø–æ –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏ (—Å–≤–µ—Ä—Ö—É –≤–Ω–∏–∑)
        all_tech_texts.sort(key=lambda x: x['y_position'])
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        for item in all_tech_texts:
            tech_text += item['text'] + "\n"
            tech_lines.append(item['text'])
        
        print(f"üìç –°–æ–±—Ä–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: {len(all_tech_texts)}")
        print(f"üìç –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: '{tech_text[:100]}...'")
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é
        if len(tech_text.strip()) < 10:
            print("üìç –ú–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π, –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é...")
            content_tech_text = self._find_tech_requirements_by_content(raw_text)
            if content_tech_text:
                tech_lines = content_tech_text.split('\n')
                tech_text = content_tech_text
                print(f"üìç –ù–∞–π–¥–µ–Ω—ã —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é: {len(tech_lines)} —Å—Ç—Ä–æ–∫")
        
        return {'text': tech_text.strip(), 'lines': tech_lines, 'spans': tech_spans}


    def _find_tech_requirements_by_content(self, raw_text: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é"""
        lines = raw_text.split('\n')
        tech_lines = []
        in_tech_section = False
        tech_section_started = False
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        tech_start_keywords = [
            '—Ä–∞–∑–º–µ—Ä—ã', '–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å', '–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å', '–¥–æ–ø—É—Å–∫', '—à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç—å',
            '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '1 *', '2 *', '3 *', '1.', '2.', '3.'
        ]
        
        tech_content_keywords = [
            '—Ä–∞–∑–º–µ—Ä', '–æ–±—Ä–∞–±–æ—Ç', '–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç', '–¥–æ–ø—É—Å–∫', '—à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç—å',
            '–ø–æ–∫—Ä—ã—Ç–∏–µ', '–∑–∞—â–∏—Ç', '–∫–∞—á–µ—Å—Ç–≤–æ', '—Ç–æ—á–Ω–æ—Å—Ç—å', '—Å–±–æ—Ä–∫', '—Å–≤–∞—Ä'
        ]
        
        end_keywords = ['–ø—Ä–∏–º–µ—á–∞–Ω–∏—è', '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '—Ç–∞–±–ª–∏—Ü–∞', '—Ä–∏—Å—É–Ω–æ–∫', '---']
        
        for i, line in enumerate(lines):
            clean_line = line.strip()
            if not clean_line:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
            if not in_tech_section:
                # –ò—â–µ–º –Ω–∞—á–∞–ª–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏–ª–∏ –Ω—É–º–µ—Ä–∞—Ü–∏–∏
                if (any(keyword in clean_line.lower() for keyword in tech_start_keywords) or
                    re.match(r'^\d+[\.\*\)]\s', clean_line) or
                    re.match(r'^\d+\s*[\.\*\)]\s', clean_line)):
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
                    has_tech_content = any(keyword in clean_line.lower() for keyword in tech_content_keywords)
                    if has_tech_content or re.match(r'^\d+[\.\*\)]\s', clean_line):
                        in_tech_section = True
                        tech_section_started = True
                        print(f"üìç –ù–∞—á–∞–ª–æ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –Ω–∞–π–¥–µ–Ω–æ: '{clean_line}'")
            
            # –ï—Å–ª–∏ –º—ã –≤ —Ä–∞–∑–¥–µ–ª–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
            if in_tech_section:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–µ—Ü —Ä–∞–∑–¥–µ–ª–∞
                if any(end_keyword in clean_line.lower() for end_keyword in end_keywords):
                    print(f"üìç –ö–æ–Ω–µ—Ü —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: '{clean_line}'")
                    break
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ –æ–Ω–∞ –∏–º–µ–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏–ª–∏ —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
                if (any(keyword in clean_line.lower() for keyword in tech_content_keywords) or
                    re.match(r'^\d+[\.\*\)]\s', clean_line) or
                    re.match(r'^[‚Ä¢\-\*]\s', clean_line) or
                    tech_section_started):
                    
                    tech_lines.append(clean_line)
                    
                    # –ï—Å–ª–∏ —ç—Ç–æ –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –ø–æ—Å–ª–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –ø—Ä–æ–±–µ–ª–∞, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º
                    if (len(tech_lines) > 3 and 
                        len(clean_line) < 20 and 
                        not any(keyword in clean_line.lower() for keyword in tech_content_keywords) and
                        not re.match(r'^\d+[\.\*\)]\s', clean_line)):
                        break
        
        tech_text = "\n".join(tech_lines)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–∞—à–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        if tech_text and len(tech_text) > 10:
            tech_word_count = sum(1 for keyword in tech_content_keywords if keyword in tech_text.lower())
            if tech_word_count >= 1:  # –•–æ—Ç—è –±—ã –æ–¥–Ω–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
                return tech_text
        
        return ""

    def _analyze_elements(self, drawing_text: str, tech_text: str, page_num: int) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        elements = {
            'codes': [],
            'letters': [],
            'asterisks': {'single': [], 'double': [], 'triple': []},
            'dimensions': [],
            'tolerances': [],
            'bases': [],
            'arrows': [],
            'lines': [],
            'roughness': {'drawing': [], 'tech': []}  # –î–û–ë–ê–í–õ–Ø–ï–ú –®–ï–†–û–•–û–í–ê–¢–û–°–¢–ò
        }
        
        print(f"\nüîç –ê–ù–ê–õ–ò–ó –≠–õ–ï–ú–ï–ù–¢–û–í –°–¢–†–ê–ù–ò–¶–´ {page_num}:")
        print(f"   –û–±—â–∏–π —Ç–µ–∫—Å—Ç: {len(drawing_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –¢–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: '{tech_text[:100]}...'")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç–∏ –∏–∑ –æ–±–æ–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        drawing_roughness = self._extract_roughness_from_text(drawing_text)
        tech_roughness = self._extract_roughness_from_text(tech_text)
        
        elements['roughness'] = {
            'drawing': drawing_roughness,
            'tech': tech_roughness
        }
        
        print(f"   –®–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç–∏ –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ: {drawing_roughness}")
        print(f"   –®–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç–∏ –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö: {tech_roughness}")
        
        # –ö–æ–¥—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        code_patterns = [
            r'[–ê-–ØA-Z]{2,4}[\.\-]\d+[\.\-]\d+[–ê-–ØA-Z]{2,3}',
            r'[–ê-–ØA-Z]{2,4}\d+\.\d+[–ê-–ØA-Z]{2,3}',
        ]
        
        for pattern in code_patterns:
            found_codes = re.findall(pattern, drawing_text, re.IGNORECASE)
            elements['codes'].extend(found_codes)
            if found_codes:
                print(f"   üìÑ –ù–∞–π–¥–µ–Ω—ã –∫–æ–¥—ã: {found_codes}")
        
        # –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–û–ò–°–ö –ë–£–ö–í - –∏—â–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ —Å—Ç–æ—è—â–∏–µ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã
        drawing_letters = self._find_standalone_letters(drawing_text)
        tech_letters = self._find_standalone_letters(tech_text)
        
        elements['letters'] = drawing_letters
        elements['tech_letters'] = tech_letters
        
        if drawing_letters:
            print(f"   üî§ –ù–∞–π–¥–µ–Ω—ã –±—É–∫–≤–µ–Ω–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ: {drawing_letters}")
        if tech_letters:
            print(f"   üî§ –ù–∞–π–¥–µ–Ω—ã –±—É–∫–≤–µ–Ω–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö: {tech_letters}")
        
        # –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–û–ò–°–ö –ó–í–ï–ó–î–û–ß–ï–ö - –∏—Å–∫–ª—é—á–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
        all_asterisks = re.findall(r'\d+\*+|\*+\d+', drawing_text)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ç–∏–ø–∞–º –∑–≤–µ–∑–¥–æ—á–µ–∫
        for ast in all_asterisks:
            if '***' in ast:
                elements['asterisks']['triple'].append(ast)
            elif '**' in ast:
                elements['asterisks']['double'].append(ast)
            elif '*' in ast:
                elements['asterisks']['single'].append(ast)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        for ast_type in elements['asterisks']:
            elements['asterisks'][ast_type] = list(set(elements['asterisks'][ast_type]))
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for ast_type in ['single', 'double', 'triple']:
            if elements['asterisks'][ast_type]:
                ast_name = self._get_asterisk_name(ast_type)
                print(f"   ‚≠ê {ast_name}: {elements['asterisks'][ast_type]}")
        
        # –†–∞–∑–º–µ—Ä—ã - —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
        dimension_patterns = [
            r'\d+[.,]?\d*\s*[–º–º—Å–º]',  # —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è
            r'\d+[.,]?\d*\s*¬∞',        # —è–≤–Ω—ã–µ —É–≥–ª–æ–≤—ã–µ
            r'[¬±]?\d+[.,]?\d*',        # —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            r'R\d+[.,]?\d*',           # —Ä–∞–¥–∏—É—Å—ã
            r'‚åÄ\d+[.,]?\d*',          # –¥–∏–∞–º–µ—Ç—Ä—ã
            r'\d+\s*–≥—Ä–∞–¥',             # "45 –≥—Ä–∞–¥"
            r'\d+\s*deg',              # –∞–Ω–≥–ª. –≤–∞—Ä–∏–∞–Ω—Ç
        ]
        all_dimensions = []
        for pattern in dimension_patterns:
            dimensions = re.findall(pattern, drawing_text, re.IGNORECASE)
            all_dimensions.extend(dimensions)
        
        elements['dimensions'] = list(set(all_dimensions))
        
        if elements['dimensions']:
            print(f"   üìè –ù–∞–π–¥–µ–Ω—ã —Ä–∞–∑–º–µ—Ä—ã ({len(elements['dimensions'])} —à—Ç): {elements['dimensions'][:10]}")
        
        # –î–æ–ø—É—Å–∫–∏ –∏ –±–∞–∑—ã
        for symbol in Config.TOLERANCE_SYMBOLS:
            if symbol in drawing_text:
                elements['tolerances'].append(symbol)
                print(f"   ‚öôÔ∏è –ù–∞–π–¥–µ–Ω —Å–∏–º–≤–æ–ª –¥–æ–ø—É—Å–∫–∞: {symbol}")
                base_matches = re.findall(f'{re.escape(symbol)}[\\s]*([A-Z{Config.BASE_SEPARATOR}]+)', drawing_text)
                if base_matches:
                    elements['bases'].extend(base_matches)
                    print(f"   üéØ –ù–∞–π–¥–µ–Ω—ã –±–∞–∑—ã –¥–ª—è {symbol}: {base_matches}")
        
        return elements
    
    def _extract_roughness_from_text(self, text: str) -> list:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è —à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        roughness_patterns = [
            r'R[az]\s*\d+[.,]?\d*',  # Ra 3.2, Rz 50
            r'R[az]\d+[.,]?\d*',     # Ra3.2, Rz50
            r'—à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç—å\s*R[az]\s*\d+[.,]?\d*',  # —à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç—å Ra 3.2
        ]
        
        found_roughness = []
        for pattern in roughness_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç
                normalized = re.sub(r'\s+', ' ', match.strip())
                found_roughness.append(normalized)
        
        return list(set(found_roughness))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

    def _find_standalone_letters(self, text: str) -> list:
        """–ü–æ–∏—Å–∫ –æ—Ç–¥–µ–ª—å–Ω–æ —Å—Ç–æ—è—â–∏—Ö –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤ (–Ω–µ –≤ —Å–æ—Å—Ç–∞–≤–µ —Å–ª–æ–≤ –∏–ª–∏ –∫–æ–¥–æ–≤)"""
        standalone_pattern = r'(?<!\w)[A-Z–ê-–Ø](?!\w)'
        all_letters = re.findall(standalone_pattern, text)
        
        common_drawing_letters = {'A', 'B', 'C', 'D', 'X', 'Y', 'Z', 'I', 'V', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T','H','–¢','–ù'}
        
        dimension_letters = set()
        dimension_patterns = [r'R\d', r'‚åÄ\d', r'[A-Z]\d', r'\d[A-Z]']
        for pattern in dimension_patterns:
            dimension_matches = re.findall(pattern, text)
            for match in dimension_matches:
                if len(match) == 2 and match[0].isalpha():
                    dimension_letters.add(match[0])
                elif len(match) == 2 and match[1].isalpha():
                    dimension_letters.add(match[1])
        
        filtered_letters = []
        for letter in all_letters:
            if (letter not in common_drawing_letters and 
                letter not in dimension_letters and
                letter not in filtered_letters):
                filtered_letters.append(letter)
        
        return filtered_letters

    def _analyze_graphic_elements(self, drawings: list, text_dict: dict, page) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–ª–∏–Ω–∏–π, —Å—Ç—Ä–µ–ª–æ–∫)"""
        graphic_analysis = {
            'lines': [],
            'arrows': [],
            'dimension_lines': [],
            'extension_lines': [],
            'dimension_elements': [],
            'tolerance_frames': [],
            'dimension_texts': []
        }
        
        print(f"\nüìê –ê–ù–ê–õ–ò–ó –ì–†–ê–§–ò–ß–ï–°–ö–ò–• –≠–õ–ï–ú–ï–ù–¢–û–í:")
        print(f"   Drawing objects: {len(drawings)}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–∏–Ω–∏–∏ –∏–∑ drawings
        for drawing in drawings:
            items = drawing.get('items', [])
            for item in items:
                if item[0] == 'l':  # line
                    line_data = {
                        'type': 'line',
                        'start': item[1],
                        'end': item[2],
                        'length': self._calculate_distance(item[1], item[2]),
                        'angle': self._calculate_angle(item[1], item[2]),
                        'color': item[3] if len(item) > 3 else (0, 0, 0),
                        'width': item[4] if len(item) > 4 else 1.0
                    }
                    graphic_analysis['lines'].append(line_data)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–µ–ª–∫–∏ (–∫–æ—Ä–æ—Ç–∫–∏–µ –ª–∏–Ω–∏–∏ –ø–æ–¥ —É–≥–ª–æ–º)
                    if 2 <= line_data['length'] <= 8:
                        if 50 <= abs(line_data['angle']) <= 70 or 110 <= abs(line_data['angle']) <= 130:
                            graphic_analysis['arrows'].append(line_data)
        
        print(f"   üìè –õ–∏–Ω–∏–π: {len(graphic_analysis['lines'])}")
        print(f"   üèπ –°—Ç—Ä–µ–ª–æ–∫: {len(graphic_analysis['arrows'])}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –ª–∏–Ω–∏–π
        for block in text_dict.get('blocks', []):
            if block['type'] == 0:
                for line in block['lines']:
                    for span in line['spans']:
                        text = span.get('text', '').strip()
                        if not text:
                            continue
                        bbox = span['bbox']
                        position = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]

                        is_numeric = bool(re.search(r'\d', text))
                        if not is_numeric:
                            continue

                        core_text = re.sub(r'[\*\s]+$', '', text)
                        if not core_text:
                            continue

                        is_dimension = bool(re.match(
                            r'^[¬±]?\d+[.,]?\d*[¬∞–º–º—Å–ºR‚åÄ]?$|'
                            r'^R\d+[.,]?\d*$|'
                            r'^‚åÄ\d+[.,]?\d*$|'
                            r'^\d+[.,]?\d*\s*¬∞$|'
                            r'^\d+\s*(–≥—Ä–∞–¥|deg)$',
                            core_text, re.IGNORECASE
                        ))

                        if not is_dimension:
                            continue

                        is_angular = any(ind in text.lower() for ind in ['¬∞', '–≥—Ä–∞–¥', 'deg', '—É–≥–æ–ª', '‚à†'])

                        text_data = {
                            'text': text,
                            'position': position,
                            'rotation': span.get('rot', 0),
                            'font_size': span.get('size', 10),
                            'bbox': bbox,
                            'is_angular': is_angular
                        }
                        graphic_analysis['dimension_texts'].append(text_data)

                        if any(symbol in text for symbol in Config.TOLERANCE_SYMBOLS):
                            graphic_analysis['tolerance_frames'].append({
                                'text': text,
                                'position': position,
                                'rotation': span.get('rot', 0),
                                'bbox': bbox
                            })
        
        print(f"   üî¢ –†–∞–∑–º–µ—Ä–Ω—ã—Ö —á–∏—Å–µ–ª: {len(graphic_analysis['dimension_texts'])}")
        print(f"   ‚öôÔ∏è –†–∞–º–æ–∫ –¥–æ–ø—É—Å–∫–æ–≤: {len(graphic_analysis['tolerance_frames'])}")
        
        graphic_analysis['dimension_elements'] = self._analyze_dimension_elements(
            graphic_analysis['lines'], 
            graphic_analysis['dimension_texts']
        )
        
        return graphic_analysis

    def _analyze_dimension_elements(self, lines, dimension_texts):
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å fallback –ø–æ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
        dimension_elements = []
        for text_elem in dimension_texts:
            nearby_lines = []
            shelf_lines = []
            dimension_direction = None

            for line in lines:
                distance = self._distance_to_line(line['start'], line['end'], text_elem['position'])
                if distance < 20:
                    nearby_lines.append(line)
                if distance <= 8 and (abs(line['angle']) < 15 or abs(line['angle'] - 180) < 15):
                    shelf_lines.append(line)

            if nearby_lines:
                closest_line = min(nearby_lines, key=lambda l: self._distance_to_line(l['start'], l['end'], text_elem['position']))
                dimension_direction = closest_line['angle']
            else:
                dimension_direction = text_elem['rotation']

            dimension_elements.append({
                'text': text_elem['text'],
                'position': text_elem['position'],
                'rotation': text_elem['rotation'],
                'font_size': text_elem['font_size'],
                'nearby_lines': nearby_lines,
                'shelf_lines': shelf_lines,
                'dimension_direction': dimension_direction,
                'is_angular': text_elem.get('is_angular', False),
                'bbox': text_elem.get('bbox', [])
            })
        return dimension_elements

    def _is_in_30_degree_zone(self, angle):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —É–≥–æ–ª –≤ –∑–æ–Ω–µ 30¬∞ –æ—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏ –∏–ª–∏ –≤–µ—Ä—Ç–∏–∫–∞–ª–∏"""
        if angle is None:
            return False
        
        normalized_angle = angle % 180
        horizontal_zone = (0 <= normalized_angle <= 30) or (150 <= normalized_angle <= 180)
        vertical_zone = 60 <= normalized_angle <= 120
        
        return horizontal_zone or vertical_zone

    def _is_text_horizontal(self, rotation):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–º"""
        return abs(rotation) < 10 or abs(rotation - 180) < 10

    def _calculate_distance(self, point1, point2):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–æ—á–∫–∞–º–∏"""
        return math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2)

    def _calculate_angle(self, point1, point2):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–≥–ª–∞ –ª–∏–Ω–∏–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 180]"""
        dx = point2[0] - point1[0]
        dy = point2[1] - point1[1]
        if dx == 0:
            return 90
        angle = math.degrees(math.atan2(dy, dx))
        return angle if angle >= 0 else angle + 180

    def _distance_to_line(self, line_start, line_end, point):
        """–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç —Ç–æ—á–∫–∏ –¥–æ –ª–∏–Ω–∏–∏"""
        x1, y1 = line_start
        x2, y2 = line_end
        x0, y0 = point
        
        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)
        denominator = math.sqrt((y2-y1)**2 + (x2-x1)**2)
        
        return numerator / denominator if denominator != 0 else float('inf')

    def _print_detailed_diagnostics(self, analysis: dict, width: float, height: float, page_num: int=1):
        """–î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        print(f"\nüìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–¢–†–ê–ù–ò–¶–´ {page_num}:")
        print(f"   üìã –û–°–ù–û–í–ù–ê–Ø –ù–ê–î–ü–ò–°–¨: '{analysis['title_block']['text'][:100]}...'")
        print(f"   üìã –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø: '{analysis['tech_requirements']['text'][:100]}...'")
        print(f"   üìã –ü–û–õ–ï –ß–ï–†–¢–ï–ñ–ê: {len(analysis['drawing_area']['text'])} —Å–∏–º–≤–æ–ª–æ–≤")
        
        elements = analysis['found_elements']
        print(f"   üîç –ù–ê–ô–î–ï–ù–û:")
        print(f"      ‚Ä¢ –ö–æ–¥–æ–≤: {len(elements['codes'])}")
        print(f"      ‚Ä¢ –ë—É–∫–≤ –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ: {len(elements['letters'])}")
        print(f"      ‚Ä¢ –ë—É–∫–≤ –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö: {len(elements.get('tech_letters', []))}")
        print(f"      ‚Ä¢ –†–∞–∑–º–µ—Ä–æ–≤: {len(elements['dimensions'])}")
        print(f"      ‚Ä¢ –î–æ–ø—É—Å–∫–æ–≤: {len(elements['tolerances'])}")
        print(f"      ‚Ä¢ –ë–∞–∑: {len(elements['bases'])}")
        total_asterisks = sum(len(v) for v in elements['asterisks'].values())
        print(f"      ‚Ä¢ –ó–≤–µ–∑–¥–æ—á–µ–∫: {total_asterisks}")

    def _get_asterisk_name(self, ast_type: str) -> str:
        names = {
            'single': '–û–¥–∏–Ω–∞—Ä–Ω—ã–µ –∑–≤–µ–∑–¥–æ—á–∫–∏ (*)',
            'double': '–î–≤–æ–π–Ω—ã–µ –∑–≤–µ–∑–¥–æ—á–∫–∏ (**)', 
            'triple': '–¢—Ä–æ–π–Ω—ã–µ –∑–≤–µ–∑–¥–æ—á–∫–∏ (***)'
        }
        return names.get(ast_type, ast_type)

# =============================================================================
# PRECISE RULE ENGINE (—Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏ 1.1.5 –∏ 1.1.6)
# =============================================================================
class PreciseRuleEngine:
    def __init__(self):
        self.document_codes = Config.DOCUMENT_CODES

    def run_all_checks(self, document_data: dict) -> dict:
        """–¢–û–ß–ù–´–ï –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏"""
        text_data = document_data['text_data']
        
        if not text_data.get('pages'):
            return self._empty_result()

        violations = []
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        first_page_tech_requirements = text_data.get('first_page_tech_requirements', '')
        
        print(f"\nüìã –û–ë–©–ò–ï –¢–ï–•–¢–†–ï–ë–û–í–ê–ù–ò–Ø –° –ü–ï–†–í–û–ô –°–¢–†–ê–ù–ò–¶–´:")
        print(f"'{first_page_tech_requirements[:200]}...'")
        
        for page in text_data['pages']:
            analysis = page['analysis']
            page_num = page['page_number']
            
            print(f"\nüîç –ü–†–û–í–ï–†–ö–ê –°–¢–†–ê–ù–ò–¶–´ {page_num}:")
            
            # 1.1.1 - –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞
            violations.extend(self._check_1_1_1_precise(page, analysis))
            
            # 1.1.3 - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –±—É–∫–≤–µ–Ω–Ω—ã—Ö –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
            violations.extend(self._check_1_1_3_precise(page, analysis, first_page_tech_requirements))
            
            # 1.1.4 - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–≤–µ–∑–¥–æ—á–µ–∫ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
            violations.extend(self._check_1_1_4_precise(page, analysis, first_page_tech_requirements))
            
            # 1.1.5 - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –≤ –∑–æ–Ω–µ 30¬∞
            violations.extend(self._check_1_1_5_precise(page, analysis))
            
            # 1.1.6 - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —É–≥–ª–æ–≤—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
            violations.extend(self._check_1_1_6_precise(page, analysis))
            
            # 1.1.8 - –¢–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π –±–∞–∑
            violations.extend(self._check_1_1_8_precise(page, analysis))

            # –ù–û–í–ê–Ø –ü–†–û–í–ï–†–ö–ê 1.1.9
            violations.extend(self._check_1_1_9_precise(page, analysis, first_page_tech_requirements))

        
        
        print(f"\nüìà –ò–¢–û–ì–û –ù–ê–†–£–®–ï–ù–ò–ô: {len(violations)}")
        
        stats = {
            'total_violations': len(violations),
            'high_severity': len([v for v in violations if v['severity'] == 'high']),
            'medium_severity': len([v for v in violations if v['severity'] == 'medium']),
            'low_severity': len([v for v in violations if v['severity'] == 'low'])
        }

        return {
            'violations': violations,
            'statistics': stats,
            'is_compliant': len([v for v in violations if v['severity'] in ['high', 'medium']]) == 0
        }

    def _check_1_1_1_precise(self, page: dict, analysis: dict) -> list:
        """1.1.1 - –ö–û–ù–ö–†–ï–¢–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏ (–±–µ–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è)"""
        violations = []
        page_num = page['page_number']
        title_text = analysis['title_block']['text']
        found_codes = analysis['found_elements']['codes']
        
        print(f"   1.1.1 –û—Å–Ω–æ–≤–Ω–∞—è –Ω–∞–¥–ø–∏—Å—å: '{title_text[:50]}...'")
        print(f"   1.1.1 –ù–∞–π–¥–µ–Ω—ã –∫–æ–¥—ã: {found_codes}")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–¥–æ–≤
        unique_codes = list(set(found_codes))
        print(f"   1.1.1 –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–¥—ã: {unique_codes}")
        
        if not unique_codes:
            violations.append({
                'rule_id': '1.1.1',
                'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏: –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞',
                'violation': '–í –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏ –ù–ï –ù–ê–ô–î–ï–ù –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞',
                'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–∞–¥–ø–∏—Å—å',
                'severity': 'high',
                'recommendation': '–î–æ–±–∞–≤—å—Ç–µ –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: –û–†–ì–ê–ù–ò–ó–ê–¶–ò–Ø.–ù–û–ú–ï–†.–í–ï–†–°–ò–Ø–¢–ò–ü'
            })
            return violations
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–¥—ã
        for code in unique_codes:
            doc_type_match = re.search(r'[–ê-–ØA-Z]{2,3}$', code)
            if not doc_type_match:
                violations.append({
                    'rule_id': '1.1.1',
                    'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏: —Ñ–æ—Ä–º–∞—Ç –∫–æ–¥–∞',
                    'violation': f'–ö–æ–¥ "{code}" –∏–º–µ–µ—Ç –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç',
                    'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}',
                    'severity': 'high',
                    'recommendation': '–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–æ—Ä–º–∞—Ç —Å 2-3 –±—É–∫–≤–∞–º–∏ —Ç–∏–ø–∞ –≤ –∫–æ–Ω—Ü–µ'
                })
                continue
            
            doc_type = doc_type_match.group()
            
            if doc_type not in self.document_codes:
                violations.append({
                    'rule_id': '1.1.1',
                    'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏: —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞',
                    'violation': f'–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ "{doc_type}" –≤ –∫–æ–¥–µ "{code}" –ù–ï –°–£–©–ï–°–¢–í–£–ï–¢ –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–µ',
                    'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, –∫–æ–¥: {code}',
                    'severity': 'high',
                    'recommendation': f'–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–∏–ø—ã: {", ".join(self.document_codes.keys())}'
                })
                continue
            
            expected_name = self.document_codes[doc_type]
            
            if expected_name.lower() not in title_text.lower():
                violations.append({
                    'rule_id': '1.1.1',
                    'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏: —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–¥–∞ –∏ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è',
                    'violation': f'–ö–æ–¥ "{code}" —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ "{expected_name}", –Ω–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏ —ç—Ç–æ –ù–ï –£–ö–ê–ó–ê–ù–û',
                    'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–∞–¥–ø–∏—Å—å',
                    'severity': 'high',
                    'recommendation': f'–ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–¥–ø–∏—Å–∏ –Ω–∞: "{expected_name}"'
                })
        
        return violations

    def _check_1_1_3_precise(self, page: dict, analysis: dict, first_page_tech_requirements: str) -> list:
        """1.1.3 - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –±—É–∫–≤–µ–Ω–Ω—ã—Ö –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã)"""
        violations = []
        page_num = page['page_number']
        drawing_letters = analysis['found_elements']['letters']
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        tech_letters = self._find_standalone_letters(first_page_tech_requirements)
        
        print(f"   1.1.3 –ë—É–∫–≤—ã –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ (—Å—Ç—Ä. {page_num}): {drawing_letters}")
        print(f"   1.1.3 –ë—É–∫–≤—ã –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö (—Å 1 —Å—Ç—Ä.): {tech_letters}")
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –±—É–∫–≤ –Ω–∏ –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ, –Ω–∏ –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö - —ç—Ç–æ –Ω–µ –æ—à–∏–±–∫–∞
        if not drawing_letters and not tech_letters:
            print(f"   1.1.3 –ë—É–∫–≤–µ–Ω–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è - –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞")
            return violations
        
        # –°–ª—É—á–∞–π 1: –ë—É–∫–≤—ã –µ—Å—Ç—å –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ, –Ω–æ –Ω–µ—Ç —Ä–∞–∑–¥–µ–ª–∞ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        if drawing_letters and not first_page_tech_requirements.strip():
            violations.append({
                'rule_id': '1.1.3',
                'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –±—É–∫–≤–µ–Ω–Ω—ã—Ö –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π',
                'violation': f'–ù–∞ —á–µ—Ä—Ç–µ–∂–µ –µ—Å—Ç—å –±—É–∫–≤—ã {", ".join(drawing_letters)}, –Ω–æ –†–ê–ó–î–ï–õ–ê —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ù–ï–¢ –¥–ª—è –∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏—è',
                'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}',
                'severity': 'medium',
                'recommendation': '–î–æ–±–∞–≤—å—Ç–µ —Ä–∞–∑–¥–µ–ª "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è" –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –±—É–∫–≤—ã'
            })
            return violations
        
        # –°–ª—É—á–∞–π 2: –ë—É–∫–≤—ã –µ—Å—Ç—å –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ, –Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö
        missing_in_tech = []
        for letter in drawing_letters:
            if letter not in tech_letters:
                missing_in_tech.append(letter)
        
        if missing_in_tech:
            violations.append({
                'rule_id': '1.1.3',
                'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –±—É–∫–≤–µ–Ω–Ω—ã—Ö –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π',
                'violation': f'–ë—É–∫–≤—ã {", ".join(missing_in_tech)} –ò–°–ü–û–õ–¨–ó–£–Æ–¢–°–Ø –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ (—Å—Ç—Ä. {page_num}), –Ω–æ –ù–ï –ü–û–Ø–°–ù–ï–ù–´ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ',
                'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (—Å—Ç—Ä. 1)',
                'severity': 'medium',
                'recommendation': f'–î–æ–±–∞–≤—å—Ç–µ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è –¥–ª—è –±—É–∫–≤: {", ".join(missing_in_tech)}'
            })
        
        # –°–ª—É—á–∞–π 3: –ë—É–∫–≤—ã –µ—Å—Ç—å –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö, –Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ
        missing_in_drawing = []
        for letter in tech_letters:
            if letter not in drawing_letters:
                missing_in_drawing.append(letter)
        
        if missing_in_drawing:
            violations.append({
                'rule_id': '1.1.3',
                'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –±—É–∫–≤–µ–Ω–Ω—ã—Ö –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π',
                'violation': f'–ë—É–∫–≤—ã {", ".join(missing_in_drawing)} –£–ö–ê–ó–ê–ù–´ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –Ω–æ –ù–ï –ò–°–ü–û–õ–¨–ó–£–Æ–¢–°–Ø –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ (—Å—Ç—Ä. {page_num})',
                'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, –ø–æ–ª–µ —á–µ—Ä—Ç–µ–∂–∞',
                'severity': 'medium',
                'recommendation': f'–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±—É–∫–≤—ã {", ".join(missing_in_drawing)} –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ –∏–ª–∏ —É–¥–∞–ª–∏—Ç–µ –∏—Ö –∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ'
            })
        
        # –ï—Å–ª–∏ –≤—Å–µ –±—É–∫–≤—ã —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã
        if not missing_in_tech and not missing_in_drawing and (drawing_letters or tech_letters):
            print(f"   1.1.3 –í—Å–µ –±—É–∫–≤–µ–Ω–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã - –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞")
        
        return violations

    def _check_1_1_4_precise(self, page: dict, analysis: dict, first_page_tech_requirements: str) -> list:
        """1.1.4 - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–≤–µ–∑–¥–æ—á–µ–∫ (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã)"""
        violations = []
        page_num = page['page_number']
        asterisks = analysis['found_elements']['asterisks']
        
        found_any = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–≤–µ–∑–¥–æ—á–∫–∏ –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ
        for ast_type in ['single', 'double', 'triple']:
            ast_list = asterisks[ast_type]
            if ast_list:
                found_any = True
                ast_name = self._get_asterisk_name(ast_type)
                print(f"   1.1.4 –ù–∞–π–¥–µ–Ω—ã {ast_name} –Ω–∞ —Å—Ç—Ä. {page_num}: {ast_list}")
                
                if not first_page_tech_requirements.strip():
                    violations.append({
                        'rule_id': '1.1.4',
                        'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–≤–µ–∑–¥–æ—á–µ–∫',
                        'violation': f'{ast_name} {", ".join(ast_list)} –µ—Å—Ç—å –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ (—Å—Ç—Ä. {page_num}), –Ω–æ –†–ê–ó–î–ï–õ–ê —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ù–ï–¢ –¥–ª—è –∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏—è',
                        'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, –ø–æ–ª–µ —á–µ—Ä—Ç–µ–∂–∞',
                        'severity': 'medium',
                        'recommendation': f'–î–æ–±–∞–≤—å—Ç–µ —Ä–∞–∑–¥–µ–ª "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è" –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏ –¥–ª—è {ast_name.lower()}'
                    })
                else:
                    tech_asterisks = self._count_tech_asterisks(first_page_tech_requirements, ast_type)
                    if tech_asterisks == 0:
                        violations.append({
                            'rule_id': '1.1.4',
                            'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–≤–µ–∑–¥–æ—á–µ–∫',
                            'violation': f'{ast_name} {", ".join(ast_list)} –µ—Å—Ç—å –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ (—Å—Ç—Ä. {page_num}), –Ω–æ –û–¢–°–£–¢–°–¢–í–£–Æ–¢ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ',
                            'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (—Å—Ç—Ä. 1)',
                            'severity': 'medium',
                            'recommendation': f'–î–æ–±–∞–≤—å—Ç–µ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è –¥–ª—è {ast_name.lower()}'
                        })
        
        # –ù–û–í–ê–Ø –ü–†–û–í–ï–†–ö–ê: –∑–≤–µ–∑–¥–æ—á–∫–∏ –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –±–µ–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ
        for ast_type in ['single', 'double', 'triple']:
            tech_asterisks_count = self._count_tech_asterisks(first_page_tech_requirements, ast_type)
            drawing_asterisks_count = len(asterisks[ast_type])
            
            if tech_asterisks_count > 0 and drawing_asterisks_count == 0:
                ast_name = self._get_asterisk_name(ast_type)
                violations.append({
                    'rule_id': '1.1.4',
                    'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∑–≤–µ–∑–¥–æ—á–µ–∫',
                    'violation': f'{ast_name} —É–∫–∞–∑–∞–Ω—ã –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ (—Å—Ç—Ä. {page_num})',
                    'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (—Å—Ç—Ä. 1)',
                    'severity': 'medium',
                    'recommendation': f'–î–æ–±–∞–≤—å—Ç–µ –Ω–∞ —á–µ—Ä—Ç–µ–∂ (—Å—Ç—Ä. {page_num}) —Ä–∞–∑–º–µ—Ä—ã —Å {ast_name.lower()} –∏–ª–∏ —É–¥–∞–ª–∏—Ç–µ –∏—Ö –∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ'
                })
        
        if not found_any:
            print(f"   1.1.4 –ó–≤–µ–∑–¥–æ—á–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä. {page_num}")
        
        return violations

    def _check_1_1_5_precise(self, page: dict, analysis: dict) -> list:
        """1.1.5 - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –≤ –∑–æ–Ω–µ 30¬∞: —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–º"""
        violations = []
        page_num = page['page_number']
        graphic_analysis = analysis['graphic_analysis']
        dimension_elements = graphic_analysis['dimension_elements']
        print(f"   1.1.5 –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä. {page_num}: –≤—Å–µ–≥–æ {len(dimension_elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        if not dimension_elements:
            print(f"   1.1.5 –†–∞–∑–º–µ—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä. {page_num}")
            return violations

        analyzer = DocumentAnalyzer()
        for i, element in enumerate(dimension_elements):
            if element.get('is_angular', False):
                continue

            dimension_direction = element.get('dimension_direction')
            text_rotation = element.get('rotation', 0)
            position = element.get('position', [0, 0])
            
            if dimension_direction is None:
                dimension_direction = text_rotation

            in_zone = analyzer._is_in_30_degree_zone(dimension_direction)
            is_horizontal = analyzer._is_text_horizontal(text_rotation)
            
            print(f"      [{i+1}] –†–∞–∑–º–µ—Ä: '{element['text']}', –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {dimension_direction:.1f}¬∞, "
                  f"–ø–æ–≤–æ—Ä–æ—Ç —Ç–µ–∫—Å—Ç–∞: {text_rotation:.1f}¬∞, –≤ –∑–æ–Ω–µ 30¬∞: {in_zone}, –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–µ–Ω: {is_horizontal}")

            if in_zone and not is_horizontal:
                violations.append({
                    'rule_id': '1.1.5',
                    'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –Ω–∞ –ø–æ–ª–∫–µ –ª–∏–Ω–∏–∏ –≤—ã–Ω–æ—Å–∫–∏ –ø—Ä–∏ –∏—Ö –ø–æ–ø–∞–¥–∞–Ω–∏–∏ –≤ –∑–æ–Ω—É 30¬∞',
                    'violation': f'–†–∞–∑–º–µ—Ä "{element["text"]}" –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∑–æ–Ω–µ 30¬∞, –Ω–æ —Ç–µ–∫—Å—Ç –Ω–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–µ–Ω (—É–≥–æ–ª: {text_rotation:.1f}¬∞)',
                    'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã ({position[0]:.1f}, {position[1]:.1f})',
                    'severity': 'medium',
                    'recommendation': '–í –∑–æ–Ω–µ 30¬∞ –æ—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏/–≤–µ—Ä—Ç–∏–∫–∞–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω—ã–µ —á–∏—Å–ª–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ.'
                })
        return violations

    def _check_1_1_6_precise(self, page: dict, analysis: dict) -> list:
        """1.1.6 - –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≥–ª–æ–≤—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –≤ –∑–æ–Ω–µ 30¬∞: —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–º"""
        violations = []
        page_num = page['page_number']
        graphic_analysis = analysis['graphic_analysis']
        dimension_elements = graphic_analysis['dimension_elements']
        print(f"   1.1.6 –ê–Ω–∞–ª–∏–∑ —É–≥–ª–æ–≤—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä. {page_num}: –≤—Å–µ–≥–æ {len(dimension_elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        if not dimension_elements:
            return violations

        analyzer = DocumentAnalyzer()
        angular_elements = [elem for elem in dimension_elements if elem.get('is_angular', False)]
        print(f"      –ù–∞–π–¥–µ–Ω–æ —É–≥–ª–æ–≤—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä. {page_num}: {len(angular_elements)}")
        
        for i, element in enumerate(angular_elements):
            dimension_direction = element.get('dimension_direction')
            text_rotation = element.get('rotation', 0)
            position = element.get('position', [0, 0])
            
            if dimension_direction is None:
                dimension_direction = text_rotation

            in_zone = analyzer._is_in_30_degree_zone(dimension_direction)
            is_horizontal = analyzer._is_text_horizontal(text_rotation)
            
            print(f"      [{i+1}] –£–≥–ª–æ–≤–æ–π —Ä–∞–∑–º–µ—Ä: '{element['text']}', –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {dimension_direction:.1f}¬∞, "
                  f"–ø–æ–≤–æ—Ä–æ—Ç —Ç–µ–∫—Å—Ç–∞: {text_rotation:.1f}¬∞, –≤ –∑–æ–Ω–µ 30¬∞: {in_zone}, –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–µ–Ω: {is_horizontal}")

            if in_zone and not is_horizontal:
                violations.append({
                    'rule_id': '1.1.6',
                    'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —É–≥–ª–æ–≤—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –Ω–∞ –ø–æ–ª–∫–µ –ª–∏–Ω–∏–∏ –≤—ã–Ω–æ—Å–∫–∏ –ø—Ä–∏ –∏—Ö –ø–æ–ø–∞–¥–∞–Ω–∏–∏ –≤ –∑–æ–Ω—É 30¬∞',
                    'violation': f'–£–≥–ª–æ–≤–æ–π —Ä–∞–∑–º–µ—Ä "{element["text"]}" –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∑–æ–Ω–µ 30¬∞, –Ω–æ —Ç–µ–∫—Å—Ç –Ω–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–µ–Ω (—É–≥–æ–ª: {text_rotation:.1f}¬∞)',
                    'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã ({position[0]:.1f}, {position[1]:.1f})',
                    'severity': 'medium',
                    'recommendation': '–£–≥–ª–æ–≤—ã–µ —Ä–∞–∑–º–µ—Ä—ã –≤ –∑–æ–Ω–µ 30¬∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ –Ω–∞ –ø–æ–ª–∫–µ –ª–∏–Ω–∏–∏-–≤—ã–Ω–æ—Å–∫–∏.'
                })
        return violations

    def _check_1_1_8_precise(self, page: dict, analysis: dict) -> list:
        """1.1.8 - –¢–û–ß–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π –±–∞–∑"""
        return self.check_datum_letter_consistency(page, analysis)

    def check_datum_letter_consistency(self, page: dict, analysis: dict) -> list:
        """1.1.8 - –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –±—É–∫–≤–µ–Ω–Ω—ã—Ö –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π –±–∞–∑"""
        violations = []
        page_num = page['page_number']
        
        print(f"   1.1.8 –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π –±–∞–∑ –Ω–∞ —Å—Ç—Ä. {page_num}")
        
        all_capital_letters = analysis['found_elements']['letters']
        print(f"   1.1.8 –í—Å–µ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ (—Å—Ç—Ä. {page_num}): {all_capital_letters}")
        
        declared_bases = self._find_bases_by_surrounding_graphics(page, analysis, all_capital_letters)
        print(f"   1.1.8 –û–±—ä—è–≤–ª–µ–Ω–Ω—ã–µ –±–∞–∑—ã –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ (—Å—Ç—Ä. {page_num}): {declared_bases}")
        
        if not declared_bases:
            print(f"   1.1.8 –ë–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä. {page_num} - –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞")
            return violations
        
        base_counts = {}
        for base in declared_bases:
            base_counts[base] = base_counts.get(base, 0) + 1
        
        print(f"   1.1.8 –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–∑ –ø–æ —Ç–∏–ø–∞–º –Ω–∞ —Å—Ç—Ä. {page_num}: {base_counts}")
        
        bases_without_pairs = []
        for base_letter, count in base_counts.items():
            if count < 2:
                bases_without_pairs.append(base_letter)
        
        if bases_without_pairs:
            violations.append({
                'rule_id': '1.1.8',
                'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –±—É–∫–≤–µ–Ω–Ω—ã—Ö –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π –±–∞–∑',
                'violation': f'–î–ª—è –±–∞–∑ {", ".join(bases_without_pairs)} –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–ª—å–∫–æ –ø–æ –æ–¥–Ω–æ–º—É —ç–∫–∑–µ–º–ø–ª—è—Ä—É –Ω–∞ —Å—Ç—Ä. {page_num}, —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º –¥–≤–∞',
                'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, –ø–æ–ª–µ —á–µ—Ä—Ç–µ–∂–∞',
                'severity': 'medium',
                'recommendation': f'–î–æ–±–∞–≤—å—Ç–µ –≤—Ç–æ—Ä–æ–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –±–∞–∑: {", ".join(bases_without_pairs)}'
            })
        else:
            print(f"   1.1.8 –í—Å–µ –±–∞–∑—ã –∏–º–µ—é—Ç –ø–∞—Ä—ã –Ω–∞ —Å—Ç—Ä. {page_num} - –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞")
        
        return violations

    def _find_bases_by_surrounding_graphics(self, page: dict, analysis: dict, letters: list) -> list:
        """–ò—â–µ—Ç –±–∞–∑—ã –ø–æ –Ω–∞–ª–∏—á–∏—é –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤–æ–∫—Ä—É–≥ –±—É–∫–≤"""
        bases = []
        text_dict = page.get('text_dict', {})
        drawings = page.get('drawings', [])
        
        print(f"   1.1.8 –ê–Ω–∞–ª–∏–∑ –±—É–∫–≤ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        
        letter_instances = []
        for block in text_dict.get('blocks', []):
            if block['type'] == 0:
                for line in block['lines']:
                    for span in line['spans']:
                        text = span.get('text', '').strip()
                        bbox = span.get('bbox', [])
                        
                        if text in letters:
                            center_x = (bbox[0] + bbox[2]) / 2
                            center_y = (bbox[1] + bbox[3]) / 2
                            letter_instances.append({
                                'letter': text,
                                'position': (center_x, center_y),
                                'bbox': bbox
                            })
                            print(f"   1.1.8 –ë—É–∫–≤–∞ '{text}' –≤ –ø–æ–∑–∏—Ü–∏–∏ ({center_x:.1f}, {center_y:.1f})")
        
        print(f"   1.1.8 –ù–∞–π–¥–µ–Ω–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –±—É–∫–≤: {len(letter_instances)}")
        
        for instance in letter_instances:
            letter = instance['letter']
            position = instance['position']
            
            graphic_elements_count = self._count_nearby_graphic_elements(drawings, position, 20.0)
            
            print(f"   1.1.8 –ë—É–∫–≤–∞ '{letter}' –≤ ({position[0]:.1f}, {position[1]:.1f}): {graphic_elements_count} –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ä—è–¥–æ–º")
            
            if graphic_elements_count > 0:
                bases.append(letter)
                print(f"   1.1.8 –ë—É–∫–≤–∞ '{letter}' –ø—Ä–∏–∑–Ω–∞–Ω–∞ –±–∞–∑–æ–π")
        
        return bases

    def _count_nearby_graphic_elements(self, drawings: list, point: tuple, radius: float) -> int:
        """–°—á–∏—Ç–∞–µ—Ç –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ —Ä–∞–¥–∏—É—Å–µ –æ—Ç —Ç–æ—á–∫–∏"""
        count = 0
        point_x, point_y = point
        
        for drawing in drawings:
            items = drawing.get('items', [])
            for item in items:
                if item[0] == 'l':
                    start = item[1]
                    end = item[2]
                    
                    dist_start = self._calculate_distance(start, point)
                    dist_end = self._calculate_distance(end, point)
                    dist_line = self._distance_to_line(start, end, point)
                    
                    if dist_start <= radius or dist_end <= radius or dist_line <= radius:
                        count += 1
                
                elif item[0] == 're':
                    rect = item[1]
                    center_rect = ((rect[0] + rect[2])/2, (rect[1] + rect[3])/2)
                    distance = self._calculate_distance(center_rect, point)
                    if distance <= radius:
                        count += 1
        
        return count

    def _calculate_distance(self, point1, point2):
        return math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2)

    def _distance_to_line(self, line_start, line_end, point):
        x1, y1 = line_start
        x2, y2 = line_end
        x0, y0 = point
        
        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)
        denominator = math.sqrt((y2-y1)**2 + (x2-x1)**2)
        
        return numerator / denominator if denominator != 0 else float('inf')

    def _find_standalone_letters(self, text: str) -> list:
        """–ü–æ–∏—Å–∫ –æ—Ç–¥–µ–ª—å–Ω–æ —Å—Ç–æ—è—â–∏—Ö –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤"""
        standalone_pattern = r'(?<!\w)[A-Z–ê-–Ø](?!\w)'
        all_letters = re.findall(standalone_pattern, text)
        
        common_drawing_letters = {'A', 'B', 'C', 'D', 'X', 'Y', 'Z', 'I', 'V', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T','H','–¢','–ù'}
        
        dimension_letters = set()
        dimension_patterns = [r'R\d', r'‚åÄ\d', r'[A-Z]\d', r'\d[A-Z]']
        for pattern in dimension_patterns:
            dimension_matches = re.findall(pattern, text)
            for match in dimension_matches:
                if len(match) == 2 and match[0].isalpha():
                    dimension_letters.add(match[0])
                elif len(match) == 2 and match[1].isalpha():
                    dimension_letters.add(match[1])
        
        filtered_letters = []
        for letter in all_letters:
            if (letter not in common_drawing_letters and 
                letter not in dimension_letters and
                letter not in filtered_letters):
                filtered_letters.append(letter)
        
        return filtered_letters
    

    def _check_1_1_9_precise(self, page: dict, analysis: dict, first_page_tech_requirements: str) -> list:
        """1.1.9 - –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∑–Ω–∞–∫–∞ ‚àö –≤ —Å–∫–æ–±–∫–∞—Ö –≤ —É–≥–ª—É —à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç–∏"""
        violations = []
        page_num = page['page_number']
        
        roughness_data = analysis['found_elements'].get('roughness', {})
        drawing_roughness = roughness_data.get('drawing', [])
        tech_roughness = roughness_data.get('tech', [])
        
        # –°–ª—É—á–∞–π: —à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç—å –µ—Å—Ç—å –ò –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ, –ò –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö
        if drawing_roughness and tech_roughness:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –û–ë–ï–ò–• —Å–∫–æ–±–æ–∫ –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö (–¥–∞–∂–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö)
            has_opening = '(' in first_page_tech_requirements
            has_closing = ')' in first_page_tech_requirements
            has_both_brackets = has_opening and has_closing
            
            if not has_both_brackets:
                violations.append({
                    'rule_id': '1.1.9',
                    'rule_text': '–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∑–Ω–∞–∫–∞ ‚àö –≤ —Å–∫–æ–±–∫–∞—Ö –≤ —É–≥–ª—É —à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç–∏',
                    'violation': f'–®–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç—å —É–∫–∞–∑–∞–Ω–∞ –∏ –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ ({", ".join(drawing_roughness)}), '
                                 f'–∏ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö ({", ".join(tech_roughness)}), '
                                 f'–Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å–∫–æ–±–∫–∏ "(...)" –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö',
                    'location': f'–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è',
                    'severity': 'medium',
                    'recommendation': '–î–æ–±–∞–≤—å—Ç–µ –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫–∏ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: "Ra 12,5 (‚àö)" –∏–ª–∏ —Ö–æ—Ç—è –±—ã "Ra 12,5 (...)"'
                })
            else:
                print(f"   1.1.9 –°–∫–æ–±–∫–∏ –Ω–∞–π–¥–µ–Ω—ã ‚Äî –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞")
        
        # –í–æ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö ‚Äî –Ω–∞—Ä—É—à–µ–Ω–∏—è –Ω–µ—Ç (–ø—É–Ω–∫—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ)
        return violations

    def _has_roughness_checkmark(self, tech_text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∑–Ω–∞–∫–∞ ‚àö –∏–ª–∏ –ø–∞—Ä—ã —Å–∫–æ–±–æ–∫ () –≤ —Ç–µ—Ö—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö —Ä—è–¥–æ–º —Å —à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç—å—é"""
        if not tech_text:
            return False
        
        # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∏—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫–æ–±–æ–∫
        clean_text = tech_text.replace('\r', ' ')
        lines = tech_text.split('\n')
        full_text_no_newlines = ' '.join(lines)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å —è–≤–Ω—ã–º ‚àö –∏–ª–∏ \sqrt
        explicit_patterns = [
            r'R[az]\s*\d+[.,]?\d*\s*\(‚àö\)',
            r'R[az]\s*\d+[.,]?\d*\s*\(\\sqrt\)',
            r'R[az].*?‚àö',
            r'R[az].*?\\sqrt',
        ]
        for pattern in explicit_patterns:
            if re.search(pattern, full_text_no_newlines, re.IGNORECASE):
                return True
        
        # üîπ –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –µ—Å–ª–∏ –µ—Å—Ç—å "Ra ... (" –∏ –≥–¥–µ-—Ç–æ –¥–∞–ª—å—à–µ ")"
        # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É —Å "R[az] ... ("
        has_opening = False
        has_closing = False
        for line in lines:
            line_clean = line.strip()
            if re.search(r'R[az]\s*\d+[.,]?\d*\s*\(', line_clean, re.IGNORECASE):
                has_opening = True
            if ')' in line_clean:
                has_closing = True
        
        if has_opening and has_closing:
            print("   1.1.9 –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—Ç–∫—Ä—ã–≤–∞—é—â–∞—è –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∏ ‚Üí —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ ‚àö –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            return True
        
        return False


    def _contains_checkmark_indicator(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä–æ–∫—É –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∑–Ω–∞–∫–∞ –∫–æ—Ä–Ω—è"""
        checkmark_indicators = [
            '‚àö', '\\sqrt', 'v', 'V', '‚úî', '‚úì', '‚à®', '‚àß'
        ]
        
        for indicator in checkmark_indicators:
            if indicator in text:
                print(f"   1.1.9 –ù–∞–π–¥–µ–Ω –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –∑–Ω–∞–∫–∞ –∫–æ—Ä–Ω—è: '{indicator}' –≤ —Ç–µ–∫—Å—Ç–µ: '{text}'")
                return True
        
        return False


    def _get_asterisk_name(self, ast_type: str) -> str:
        names = {
            'single': '–û–¥–∏–Ω–∞—Ä–Ω—ã–µ –∑–≤–µ–∑–¥–æ—á–∫–∏ (*)',
            'double': '–î–≤–æ–π–Ω—ã–µ –∑–≤–µ–∑–¥–æ—á–∫–∏ (**)', 
            'triple': '–¢—Ä–æ–π–Ω—ã–µ –∑–≤–µ–∑–¥–æ—á–∫–∏ (***)'
        }
        return names.get(ast_type, ast_type)

    def _count_tech_asterisks(self, tech_text: str, ast_type: str) -> int:
        patterns = {
            'single': r'(?<!\*)\*(?!\*)',
            'double': r'(?<!\*)\*\*(?!\*)',
            'triple': r'(?<!\*)\*\*\*(?!\*)'
        }
        return len(re.findall(patterns[ast_type], tech_text))

    def _empty_result(self):
        return {
            'violations': [{
                'rule_id': 'no_data',
                'rule_text': '–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö',
                'violation': '–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ PDF',
                'location': '–í–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç',
                'severity': 'high',
                'recommendation': '–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª'
            }],
            'statistics': {'total_violations': 1, 'high_severity': 1, 'medium_severity': 0, 'low_severity': 0},
            'is_compliant': False
        }

# =============================================================================
# FLASK APPLICATION
# =============================================================================
doc_analyzer = DocumentAnalyzer()
rule_engine = PreciseRuleEngine()

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() == 'pdf'

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze_document():
    if 'file' not in request.files:
        return jsonify({'error': '–§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω'}), 400
        
    file = request.files['file']
    if file.filename == '' or not allowed_file(file.filename):
        return jsonify({'error': '–¢—Ä–µ–±—É–µ—Ç—Å—è PDF-—Ñ–∞–π–ª'}), 400

    filename = secure_filename(file.filename)
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{uuid.uuid4()}_{filename}")
    file.save(file_path)
    
    try:
        text_data = doc_analyzer.extract_text_from_pdf(file_path)
        document_data = {'text_data': text_data}
        result = rule_engine.run_all_checks(document_data)
        
        os.remove(file_path)
        
        return jsonify({
            'success': True,
            'result': result
        })
        
    except Exception as e:
        if os.path.exists(file_path):
            os.remove(file_path)
        return jsonify({'error': f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}'}), 500

if __name__ == '__main__':
    print("üéØ –£–õ–£–ß–®–ï–ù–ù–´–ô NormControl –∑–∞–ø—É—â–µ–Ω!")
    print("üìã –í—Å–µ 8 –ø—Ä–æ–≤–µ—Ä–æ–∫ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π")
    print("üîç –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å –≤–∫–ª—é—á–µ–Ω")
    app.run(host='0.0.0.0', port=5000, debug=True)